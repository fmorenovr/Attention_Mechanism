{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Soft Attention PlacePulse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORFLOW 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6579f534729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import random\n",
    "import skimage.transform\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from vis.visualization import visualize_cam_with_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 224\n",
    "MAX_LENGTH = 9\n",
    "delta = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.5]\n",
    "delta_i = delta[-1]\n",
    "\n",
    "PATH = os.path.abspath('.') + \"/dataset/PlacePulse\"\n",
    "IMG_PATH = PATH + \"/images/2011/\"\n",
    "LABEL_PATH = PATH + \"/labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, img_dims=[224, 224]):\n",
    "    orig = load_img(image_path, target_size=img_dims)\n",
    "    img = img_to_array(orig)\n",
    "    #img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    #img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(delta_i, labels_path=LABEL_PATH+\"features.csv\"):\n",
    "    data = pd.read_csv(labels_path)\n",
    "    data = data.loc[:,[\"ID\",\"y\"]].copy()\n",
    "    slen = len(data)\n",
    "    val = round(delta_i*slen)\n",
    "    data[\"class\"] = data['y']\n",
    "    data['class'].iloc[:val+1]=1\n",
    "    data['class'].iloc[slen-val:] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_labels(delta_i)\n",
    "print(data[data[\"ID\"]==4340])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img = np.asarray([IMG_PATH+str(img)+\".jpg\" for img in data.loc[:,\"ID\"].values.copy()])\n",
    "all_labels = data.loc[:,\"class\"].copy()\n",
    "all_scores = data.loc[:,\"y\"].values.copy(), \n",
    "all_features = []\n",
    "\n",
    "for img in all_img:\n",
    "  features, _ = load_image(img, img_dims=[IMG_SIZE, IMG_SIZE])\n",
    "  all_features.append(features)\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "all_labels = tf.keras.utils.to_categorical(all_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slen = len(data)\n",
    "val = round(delta_i*slen)\n",
    "X_pos = all_features[:val+1]\n",
    "X_neg = all_features[slen-val:]\n",
    "y_pos = all_labels[:val+1]\n",
    "y_neg = all_labels[slen-val:]\n",
    "img_pos = all_img[:val+1]\n",
    "img_neg = all_img[slen-val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "xtrain_pos, xtest_pos, ytrain_pos, ytest_pos = train_test_split(X_pos, y_pos, shuffle=True, test_size = 0.25, random_state=35)\n",
    "xtrain_neg, xtest_neg, ytrain_neg, ytest_neg = train_test_split(X_neg, y_neg, shuffle=True, test_size = 0.25, random_state=35)\n",
    "\n",
    "train_x = np.concatenate([xtrain_pos, xtrain_neg])/255.\n",
    "test_x = np.concatenate([xtest_pos, xtest_neg])/255.\n",
    "\n",
    "imgtrain_pos, imgtest_pos, ytrain_pos, ytest_pos = train_test_split(img_pos, y_pos, shuffle=True, test_size = 0.25, random_state=35)\n",
    "imgtrain_neg, imgtest_neg, ytrain_neg, ytest_neg = train_test_split(img_neg, y_neg, shuffle=True, test_size = 0.25, random_state=35)\n",
    "\n",
    "imgtrain_val = np.concatenate([imgtrain_pos, imgtrain_neg])\n",
    "imgtest = np.concatenate([imgtest_pos, imgtest_neg])\n",
    "\n",
    "train_y = np.float32(np.concatenate([ytrain_pos, ytrain_neg]))\n",
    "test_y = np.float32(np.concatenate([ytest_pos, ytest_neg]))\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "print(train_y.shape, test_y.shape)\n",
    "print(imgtrain_val.shape, imgtest.shape)\n",
    "print(np.max(train_x), np.min(train_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "img_size = train_x.shape[1]\n",
    "img_flat_size = img_size * img_size\n",
    "\n",
    "# If you want to train the model -> True, otherwise -> False\n",
    "Is_train = True\n",
    "\n",
    "# If you want to load saved model -> True, otherwise -> False \n",
    "Load_model = False\n",
    "\n",
    "# Name of the save file\n",
    "SAVE_PATH = 'saved_model'\n",
    "\n",
    "# Numbers of sampling to test the code \n",
    "num_test_sample = 50\n",
    "\n",
    "# labels: 0 - 9\n",
    "num_label = 2\n",
    "\n",
    "# Parameters for training\n",
    "num_epoch = 60\n",
    "\n",
    "learning_rate = 5e-4\n",
    "epsilon = 1e-8\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Parameter for LSTM\n",
    "lstm_size = 256\n",
    "step_size = 4\n",
    "flatten_size = img_size\n",
    "\n",
    "gpu_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Sample Image (Modified MNIST for Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting example image\n",
    "img = train_x[0, :, :, :]\n",
    "print(img.shape)\n",
    "\n",
    "#plt.imshow(img)#, cmap = 'gray')\n",
    "#plt.show()\n",
    "print('Label: ' + str(train_y[0,:]))\n",
    "print('Shape: ' + str(img_size) + 'x' + str(img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias \n",
    "def conv2d(x,w, stride):\n",
    "\treturn tf.nn.conv2d(x,w,strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Get Variables\n",
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name,shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    return tf.get_variable(name,shape=shape, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and Attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# LSTM function\n",
    "def LSTM_cell(C_prev, h_prev, x_lstm, Wf, Wi, Wc, Wo, bf, bi, bc, bo):\n",
    "    # C_prev: Cell state from lstm of previous time step (shape: [batch_size, lstm_size])\n",
    "    # h_prev: output from lstm of previous time step (shape: [batch_size, lstm_size])\n",
    "    # x_lstm: input of lstm (shape: [batch_size, data_flatten_size])\n",
    "\n",
    "    input_concat = tf.concat([x_lstm, h_prev], 1)\n",
    "    f = tf.sigmoid(tf.matmul(input_concat, Wf) + bf)\n",
    "    i = tf.sigmoid(tf.matmul(input_concat, Wi) + bi)\n",
    "    c = tf.tanh(tf.matmul(input_concat, Wc) + bc)\n",
    "    o = tf.sigmoid(tf.matmul(input_concat, Wo) + bo)\n",
    "    \n",
    "    C_t = tf.multiply(f, C_prev) + tf.multiply(i, c) \n",
    "    h_t = tf.multiply(o, tf.tanh(C_t))\n",
    "    \n",
    "    return C_t, h_t # Cell state, Output\n",
    "\n",
    "# Soft Attention function\n",
    "def soft_attention(h_prev, a, Wa, Wh):\n",
    "    # h_prev: output from lstm of previous time step (shape: [batch_size, lstm_size])\n",
    "    # a: Result of CNN [batch_size, conv_size * conv_size, channel_size] \n",
    "\n",
    "    m_list = [tf.tanh(tf.matmul(a[i], Wa) + tf.matmul(h_prev, Wh)) for i in range(len(a))] \n",
    "    print(\"list\", m_list[0].get_shape())\n",
    "    m_concat = tf.concat([m_list[i] for i in range(len(a))], axis = 1)    \n",
    "    print(\"concat\", m_concat.get_shape())\n",
    "    alpha = tf.nn.softmax(m_concat) \n",
    "    z_list = [tf.multiply(a[i], tf.slice(alpha, (0, i), (-1, 1))) for i in range(len(a))]\n",
    "    z_stack = tf.stack(z_list, axis = 2)\n",
    "    z = tf.reduce_sum(z_stack, axis = 2)\n",
    "\n",
    "    return alpha, z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAttentionLayer(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(SoftAttentionLayer, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    self.W2 = tf.keras.layers.Dense(units, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    self.V = tf.keras.layers.Dense(1, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    #print(\"feature:\", features.shape)\n",
    "    #print(\"hidden:\", hidden_with_time_axis.shape)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "    #attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    #print(\"context:\", context_vector.shape)\n",
    "\n",
    "    return attention_weights, context_vector,  #alpha, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_state(batch_size, units):\n",
    "    return tf.zeros((batch_size, units), tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "# Input \n",
    "# x_image  = tf.placeholder(tf.float32, shape = [None, img_size, img_size, 3])\n",
    "# y_target = tf.placeholder(tf.float32, shape=[None, num_label])\n",
    "\n",
    "# conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(2,2),padding='same',use_bias=True, activation=tf.nn.relu, name=\"conv_1\")\n",
    "# conv_2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(2,2),padding='same',use_bias=True, activation=tf.nn.relu, name=\"conv_2\")\n",
    "# conv_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(2,2),padding='same',use_bias=True, activation=tf.nn.relu, name=\"conv_3\")\n",
    "\n",
    "# x = conv_1(x_image)\n",
    "# x = conv_2(x)\n",
    "# x = conv_3(x)\n",
    "\n",
    "# conv_size = x.get_shape()[1]\n",
    "# filters_size = x.get_shape()[3]\n",
    "# conv_flat = tf.reshape(x, [-1, conv_size*conv_size, filters_size])\n",
    "# conv_unstack = tf.unstack(conv_flat, axis = 1) #1\n",
    "# x_unstack = tf.stack(conv_unstack)\n",
    "\n",
    "# attention = SoftAttentionLayer(lstm_size)\n",
    "# LSTM_Cell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "# activation = tf.keras.layers.Dense(self._output_shape, activation=\"linear\", name=\"activation\")\n",
    "\n",
    "# h = reset_state(batch_size, lstm_size)\n",
    "# c = tf.zeros([rnn_batch_size, lstm_size], tf.float32)\n",
    "# print(\"h\", h.get_shape())\n",
    "# print(\"c\", h.get_shape())\n",
    "# for i in range(step_size):\n",
    "#     alpha, z = attention(x, c)\n",
    "#     lstm_input = tf.concat([z, c], axis=-1)\n",
    "#     h, [h, c] = LSTM_Cell(lstm_input, [h, c])\n",
    "\n",
    "# output_conv = activation(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "# Input \n",
    "x_image  = tf.placeholder(tf.float32, shape = [None, img_size, img_size, 3])\n",
    "y_target = tf.placeholder(tf.float32, shape=[None, num_label])\n",
    "\n",
    "# Convolution variables\n",
    "# w_conv1 = weight_variable('W_conv1', [3, 3, 3, 64])\n",
    "# b_conv1 = bias_variable('b_conv1', [64])\n",
    "# w_conv2 = weight_variable('W_conv2', [3, 3, 64, 256])\n",
    "# b_conv2 = bias_variable('b_conv2', [256])\n",
    "# w_conv3 = weight_variable('W_conv3', [3, 3, 256, 512])\n",
    "# b_conv3 = bias_variable('b_conv3', [512])\n",
    "\n",
    "# conv1 = tf.nn.relu(conv2d(x_image, w_conv1, 2) + b_conv1)\n",
    "# conv2 = tf.nn.relu(conv2d(conv1, w_conv2, 2) + b_conv2)\n",
    "# conv3 = tf.nn.relu(conv2d(conv2, w_conv3, 2) + b_conv3)\n",
    "\n",
    "# conv_size = conv3.get_shape()[1]\n",
    "# conv_flat = tf.reshape(conv3, [-1, conv_size*conv_size, 512])\n",
    "# conv_unstack = tf.unstack(conv_flat, axis = 1)\n",
    "\n",
    "conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(2,2),padding='same',use_bias=True, activation=\"linear\", name=\"conv_1\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(2,2),padding='same',use_bias=True, activation=\"linear\", name=\"conv_2\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "conv_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(2,2),padding='same',use_bias=True, activation=\"linear\", name=\"conv_3\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "x = tf.nn.relu(conv_1(x_image))\n",
    "x = tf.nn.relu(conv_2(x))\n",
    "x = tf.nn.relu(conv_3(x))\n",
    "\n",
    "conv_size = x.get_shape()[1]\n",
    "filters_size = x.get_shape()[3]\n",
    "conv_flat = tf.reshape(x, [-1, conv_size*conv_size, filters_size])\n",
    "conv_unstack = tf.unstack(conv_flat, axis = 1)\n",
    "\n",
    "attention = SoftAttentionLayer(lstm_size)\n",
    "LSTM_Cell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "activation = tf.keras.layers.Dense(num_label, activation=\"linear\", name=\"activation\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#LSTM Variables\n",
    "Wf = weight_variable('Wf', [512 + lstm_size, lstm_size])\n",
    "Wi = weight_variable('Wi', [512 + lstm_size, lstm_size])\n",
    "Wc = weight_variable('Wc', [512 + lstm_size, lstm_size])\n",
    "Wo = weight_variable('Wo', [512 + lstm_size, lstm_size])\n",
    "\n",
    "bf = bias_variable('bf', [lstm_size])\n",
    "bi = bias_variable('bi', [lstm_size])\n",
    "bc = bias_variable('bc', [lstm_size])\n",
    "bo = bias_variable('bo', [lstm_size]) \n",
    "\n",
    "# Attention Variables\n",
    "Wa = weight_variable('Wa', [512, 1])\n",
    "Wh = weight_variable('Wh', [lstm_size, 1])\n",
    "\n",
    "rnn_batch_size = tf.shape(x_image)[0]\n",
    "\n",
    "# Initial lstm cell state and output \n",
    "rnn_state = tf.zeros([rnn_batch_size, lstm_size], tf.float32)\n",
    "rnn_out = tf.zeros([rnn_batch_size, lstm_size], tf.float32)\n",
    "\n",
    "#################################### Attention!!! ####################################\n",
    "for i in range(step_size):\n",
    "    alpha, z = soft_attention(rnn_out, conv_unstack, Wa, Wh)\n",
    "    rnn_state, rnn_out = LSTM_cell(rnn_state, rnn_out, z, Wf, Wi, Wc, Wo, bf, bi, bc, bo)\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# Densely connect layer variables \n",
    "w_fc1 = weight_variable('w_fc1', [lstm_size, num_label])\n",
    "b_fc1 = bias_variable('b_fc1', [num_label])\n",
    "\n",
    "#output = tf.matmul(rnn_out, w_fc1)+b_fc1\n",
    "output = activation(rnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "Loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = y_target, logits = output)\n",
    "Cost = tf.reduce_mean(Loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, epsilon = epsilon).minimize(Cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_target,1), tf.argmax(output,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"x\", conv_3.output)\n",
    "print(\"conv\", conv_flat.shape)\n",
    "print(\"unstack\", len(conv_unstack), conv_unstack[2].get_shape())\n",
    "print(\"Ã¼nstack tensor\", tf.stack(conv_unstack).get_shape())\n",
    "print(\"state\", rnn_state.get_shape())\n",
    "print(\"Memory\", rnn_out.get_shape())\n",
    "print(\"alpha\", alpha.get_shape())\n",
    "print(\"z\", z.get_shape())\n",
    "print(\"output\", output.get_shape())\n",
    "#print(\"output\", output_conv.get_shape())\n",
    "#alpha, z = soft_attention(rnn_out, conv_unstack, Wa, Wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from vis.visualization import visualize_cam_with_losses, visualize_cam\n",
    "visualize_cam_with_losses(train_x[0,:,:,:], Cost, 1,conv_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpu_fraction\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file if the saved file exists\n",
    "saver = tf.train.Saver()\n",
    "if Load_model == True:\n",
    "    checkpoint = tf.train.get_checkpoint_state(SAVE_PATH+\"soft_net/\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "if Is_train == True:\n",
    "    train_data_num = train_x.shape[0]\n",
    "    test_data_num = test_x.shape[0]\n",
    "    for i in range(num_epoch):\n",
    "        # Making batches\n",
    "        random_idx = np.arange(train_data_num)\n",
    "        np.random.shuffle(random_idx)\n",
    "\n",
    "        #batch_count = 1\n",
    "    \n",
    "        for j in range(0, train_data_num, batch_size):\n",
    "            if j + batch_size < train_data_num:\n",
    "                batch_index = [j, j + batch_size]\n",
    "\n",
    "                batch_x_train = train_x[random_idx[batch_index[0]:batch_index[1]],:,:]\n",
    "                batch_y_train = train_y[random_idx[batch_index[0]:batch_index[1]],:]\n",
    "            else:\n",
    "                batch_index = [j, j + train_data_num-1]\n",
    "\n",
    "                batch_x_train = train_x[random_idx[batch_index[0]:batch_index[-1]],:,:]\n",
    "                batch_y_train = train_y[random_idx[batch_index[0]:batch_index[-1]],:]\n",
    "\n",
    "            # Make image as fractions for attention\n",
    "            train_batch = np.reshape(batch_x_train, (batch_x_train.shape[0], img_size, img_size, 3))\n",
    "            #validation_batch = np.reshape(validation_x, (validation_x.shape[0], img_size, img_size, 1))\n",
    "            \n",
    "            # Training\n",
    "            optimizer.run(feed_dict = {x_image: train_batch, y_target: batch_y_train})\n",
    "            cost = sess.run(Cost, feed_dict = {x_image: train_batch, y_target: batch_y_train})\n",
    "            acc = sess.run(accuracy, feed_dict = {x_image: train_batch, y_target: batch_y_train})\n",
    "            #val_acc = sess.run(accuracy, feed_dict = {x_image: validation_batch, y_target: validation_y})\n",
    "\n",
    "            # Print Progress\n",
    "            print(\"Epoch: \" + str(i+1) + ' / ' + \n",
    "                  \"Batch: \" + str(j) + '/' + str(train_data_num) + ' / ' + \n",
    "                  \"Cost: \" + str(cost) + ' / ' + \n",
    "                  \"Training Accuracy: \" + str(acc))# + ' / ' + )\n",
    "#                  \"Validation Accuracy: \" + str(val_acc))  \n",
    "        if (i+1)%10==0:\n",
    "            num_correct = 0\n",
    "            save_path = os.path.join(SAVE_PATH, 'soft_net')\n",
    "            if not os.path.isdir(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            file_name = os.path.join(save_path, 'soft_net_' + str(i))\n",
    "\n",
    "            #saver.save(sess, file_name)\n",
    "            print('Model is saved!!!')\n",
    "\n",
    "            print('Testing ...')\n",
    "            idx = random.sample(range(test_x.shape[0]), test_data_num)\n",
    "            for idx_sample in range(test_data_num):\n",
    "                test_x_reshape = np.reshape(test_x, ([test_x.shape[0],img_size,img_size,3]))\n",
    "                test_x_in = test_x_reshape[idx[idx_sample],:,:,:]\n",
    "                output_ = sess.run(output,feed_dict = {x_image: [test_x_in], y_target: [test_y[idx[idx_sample],:]]})\n",
    "                \n",
    "                y_test_pred = np.argmax(output_[:])\n",
    "                y_test_true = np.argmax(test_y[idx[idx_sample], :])\n",
    "                if y_test_pred == y_test_true:\n",
    "                    num_correct += 1.\n",
    "                \n",
    "            # Print Progress\n",
    "            print(\"Testing Accuracy: \" + str(num_correct/test_data_num))# + ' / ' + )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sampling test indexes\n",
    "idx = random.sample(range(test_x.shape[0]), num_test_sample)\n",
    "\n",
    "# Initialize fraction of test images and heatmap\n",
    "test_fraction = np.zeros([10, img_size, img_size, 3])\n",
    "heat_map = np.zeros([num_test_sample, img_size, img_size])\n",
    "\n",
    "num_correct = 0.\n",
    "#import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "# Test for Sampling data\n",
    "for idx_sample in range(num_test_sample):\n",
    "    # Get alpha(weight of fractions) and output for sample test data\n",
    "    test_x_reshape = np.reshape(test_x, ([test_x.shape[0],img_size,img_size,3]))\n",
    "    #test_x_in = test_x_reshape[idx[idx_sample],:,:,:]\n",
    "    test_x_in = test_x_reshape[idx_sample,:,:,:]\n",
    "    #alpha_, output_ = sess.run([alpha, output],feed_dict = {x_image: [test_x_in], y_target: [test_y[idx[idx_sample],:]]})\n",
    "    alpha_, output_ = sess.run([alpha, output],feed_dict = {x_image: [test_x_in], y_target: [test_y[idx_sample,:]]})\n",
    "    alpha_size = int(np.sqrt(alpha_.shape[1]))\n",
    "    alpha_reshape = np.reshape(alpha_, (alpha_size, alpha_size))\n",
    "    alpha_resize = skimage.transform.pyramid_expand(alpha_reshape, upscale = 16, sigma=20)  \n",
    "#     print(np.max(alpha_resize), np.min(alpha_resize))\n",
    "#     print(np.max(test_x_in), np.min(test_x_in))\n",
    "#     print(output_)\n",
    "    #print(np.max(test_x), np.min(test_x))\n",
    "\n",
    "    # Get labels for test samples\n",
    "    y_test_pred = np.argmax(output_[:])\n",
    "    y_test_true = np.argmax(test_y[idx_sample, :])\n",
    "    \n",
    "    # Draw subplot for each sample \n",
    "    f1, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(alpha_resize, cmap='gray')\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title('Attention Heatmap')\n",
    "    ax[1].imshow(test_x_in)#, cmap='gray')\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title('Prediction: ' + str(y_test_pred) + ' / ' + 'Label: ' + str(y_test_true))\n",
    "\n",
    "    # Count correct\n",
    "    if y_test_pred == y_test_true:\n",
    "        num_correct += 1.\n",
    "    \n",
    "    #print(np.max(alpha_resize), np.min(alpha_resize))\n",
    "\n",
    "# Show results \n",
    "#print(np.max(alpha_resize), np.min(alpha_resize))\n",
    "plt.show()\n",
    "print('Sample Accuracy: ' + str(num_correct / num_test_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling test indexes\n",
    "idx = random.sample(range(test_x.shape[0]), num_test_sample)\n",
    "\n",
    "# Initialize fraction of test images and heatmap\n",
    "test_fraction = np.zeros([10, img_size, img_size, 3])\n",
    "heat_map = np.zeros([num_test_sample, img_size, img_size])\n",
    "\n",
    "num_correct = 0.\n",
    "#import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "# Test for Sampling data\n",
    "for idx_sample in range(num_test_sample):\n",
    "    # Get alpha(weight of fractions) and output for sample test data\n",
    "    test_x_reshape = np.reshape(test_x, ([test_x.shape[0],img_size,img_size,3]))\n",
    "    #test_x_in = test_x_reshape[idx[idx_sample],:,:,:]\n",
    "    test_x_in = test_x_reshape[idx_sample,:,:,:]\n",
    "    #alpha_, output_ = sess.run([alpha, output],feed_dict = {x_image: [test_x_in], y_target: [test_y[idx[idx_sample],:]]})\n",
    "    conv_output = sess.run([conv3],feed_dict = {x_image: [test_x_in]})#, y_target: [test_y[idx_sample,:]]})\n",
    "#     alpha_size = int(np.sqrt(alpha_.shape[1]))\n",
    "#     alpha_reshape = np.reshape(alpha_, (alpha_size, alpha_size))\n",
    "#     alpha_resize = skimage.transform.pyramid_expand(alpha_reshape, upscale = 16, sigma=20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient for partial linearization. We only care about target visualization class. \n",
    "#y_c = tf.reduce_sum(tf.multiply(vgg.fc8, labels), axis=1)\n",
    "#print('y_c:', y_c)\n",
    "# Get last convolutional layer gradient for generating gradCAM visualization\n",
    "target_conv_layer = conv_3\n",
    "#target_conv_layer_grad = tf.gradients(y_c, target_conv_layer)[0]\n",
    "images = tf.placeholder(\"float\", [batch_size, img_size, img_size, 3])\n",
    "# Guided backpropagtion back to input layer\n",
    "gb_grad = tf.gradients(Cost, images)[0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
